<!DOCTYPE HTML>
<!--
	Editorial by HTML5 UP
	html5up.net | @ajlkn
	Free for personal and commercial use under the CCA 3.0 license (html5up.net/license)
-->
<html>
	<head>
		<title>Milan Portfolio</title>
		<meta charset="utf-8" />
		<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
		<link rel="stylesheet" href="assets/css/main.css" />
	</head>
	<body class="is-preload">

		<!-- Wrapper -->
			<div id="wrapper">

				<!-- Main -->
					<div id="main">
						<div class="inner">

							<!-- Header -->
							<header id="header">
								<a href="index.html" class="logo"><strong>Milan Garg</strong></a>
								<ul class="icons">
									<li>  <a href="https://www.linkedin.com/in/milan-garg-540421294/" target="_blank" class="icon brands fa-linkedin"><span class="label">Linkedin</span></a></li>
								</ul>
							</header>

							<!-- Content -->
								<section>
									<header class="main">
										<h1>Web Scraping using API</h1>
									</header>

									<span class="image main"><img src="images/simple_web_scraping/banner.jpg" alt="" /></span>

									<hr class="major" />

									<h2>Setup</h2>
									<p>In this blog post, we'll walk through the process of building a data scraper for Zomato, one of the leading food delivery platforms. We'll break down the process into two main steps: fetching paginated order data using Zomato's API and then extracting detailed order information for each item.									</p>
									<p>We will make use of python along with its requests package which can be used to automate web requests including scraping, testing and web automation. The post assumes basic knowledge of python and the concept of web api.</p>

									<hr class="major" />
									<h2>Prerequisites: How to copy headers for a web request.</h2>
									<p>Each request done by a web browser is captured in the network console. We can simply right click the request and select copy headers to get the headers used in the below codes. Note: You will need to login to the Zomato platform and copy the headers for a request to get auth details for running the below codes.</p>
									<p>Checkout this <a href="https://www.scraperapi.com/blog/headers-and-cookies-for-web-scraping/" target="_blank">link </a> for more information on how to get headers for requests. </p>
									<p><strong>Python libraries used: </strong> Requests; Pandas; JSON; dateutil; re; pprint; time</p>
									
									<hr class="major" />

									<h2>Step 1: Fetching Paginated Order Data</h2>
									<p>The first step in our data scraping journey involves fetching paginated order data from Zomato's API. We start by obtaining a valid cookie from the server to authenticate our requests. Then, we iterate through the paginated order history, fetching data for each page until we reach the last page. We store the fetched data in a JSON file for further processing.									</p>

									<pre><code># Code snippet for extracting detailed order information
import requests
import json


if __name__ == "__main__":
    #get cookie from server
    headers = {'accept': '', 'accept-language': '', 'sec-ch-ua': '', 
	'sec-ch-ua-mobile': '', 'sec-ch-ua-platform': '', 'sec-fetch-dest': '', 
	'sec-fetch-mode': '', 'sec-fetch-site': '', 'x-zomato-csrft': '', 
	'Referer': '', 'Referrer-Policy': '', 'User-Agent': '',
	'cookie': ''}

    dataset = []
    total_pages = None
    current_page = 1

    while True:

      if total_pages and current_page == total_pages:
        break

      r = requests.get(url='https://www.zomato.com/webroutes/user/orders?page={0}'.format(current_page),
                     headers=headers)
      current_page += 1
      print(r.json())
      data = r.json()
      total_pages = data['sections']['SECTION_USER_ORDER_HISTORY']['totalPages']
      dataset.append(data)

    filename = r'C:\Users\user1\data.json'
    f = open(filename, 'w')
    json.dump(dataset, f)</code></pre>

									<hr class="major" />

									<h2>Step 2: Extracting Detailed Order Information</h2>

									<ul>
										<li><strong>Reading Paginated Data:</strong> The script reads the JSON file (data.json) containing the paginated order data obtained from Step 1. It loads the JSON content into memory for further processing.</li>
										<li><strong>Iterating Through Orders:</strong> It iterates through each page of order data and extracts the unique identifiers (hash IDs) for each order item. These hash IDs are essential for fetching detailed information about each item.</li>
										<li><strong>Fetching Detailed Information:</strong> For each hash ID, the script sends a GET request to another Zomato API endpoint (https://www.zomato.com/webroutes/order/details) to fetch detailed information about the order item.</li>
										<li><strong>Data Storage:</strong> The fetched detailed information, along with the corresponding hash ID, is stored in a list called <code>dataset</code>. This list contains dictionaries, with each dictionary representing an order item and its details.</li>
										<li><strong>Rate Limiting:</strong> To avoid overloading the server, a time delay of 0.5 seconds (<code>time.sleep(0.5)</code>) is introduced between each request.</li>
										<li><strong>Final Data Storage:</strong> Once all detailed information is fetched, the entire dataset is stored in another JSON file (data_per_item.json) for further analysis.</li>
									</ul>
									<pre><code># Code snippet for extracting detailed order information
import json
import requests
import time


if __name__ == "__main__":
    #get cookie from server
    headers = {'accept': '', 'accept-language': '', 'sec-ch-ua': '', 
	'sec-ch-ua-mobile': '', 'sec-ch-ua-platform': '', 'sec-fetch-dest': '', 
	'sec-fetch-mode': '', 'sec-fetch-site': '', 'x-zomato-csrft': '', 
	'Referer': '', 'Referrer-Policy': '', 'User-Agent': '',
	'cookie': ''}

    hashlist = []

    order_file = r'C:\Users\user1\data.json'
    order_json = json.load(open(order_file,'r'))

    for _page in order_json:
        for k,v in _page['entities']['ORDER'].items():
            hashlist.append(v['hashId'])

    dataset = []

    for ind,_item in enumerate(hashlist):
        r = requests.get(url='https://www.zomato.com/webroutes/order/details?hashId={0}'.format(_item),
                         headers=headers)
        t = r.json()
        dataset.append({"hashid": _item, "data": t})
        time.sleep(0.5)

    filename = r'C:\Users\user1\data_per_item.json'
    json.dump(dataset, open(filename, 'w'))</code></pre>
								
								<hr class="major" />

								<h2>Step 3: Transforming JSON files into csv files.</h2>

								<p>The code will contain two functions one to parse the items for orders and another to parse the order data at a higher level.</p>
								
								<h3>Below steps explain the code for generating order data from orders file.</h3>
								<p><strong>Function process_order_data(datafile, outfile, per_item_file)</strong></p>
								<ul>
								<li><strong>Loading Per-Item Data:</strong> This function takes three arguments: <em>datafile</em> (path to the JSON file containing order data), <em>outfile</em> (path for the resulting CSV file), and <em>per_item_file</em> (path to the JSON file containing detailed order information per item). It loads the detailed item data from <em>per_item_file</em> into a dictionary called <em>itemdict</em>, where keys are <em>hashid</em> and values are dictionaries containing various cost-related information for each order.</li>
								<li><strong>Parsing Order Data:</strong> It iterates through each page of order data in <em>datafile</em>. For each order, it extracts relevant information such as <em>order_date</em>, <em>cost</em>, <em>payment_status</em>, <em>delivery_address</em>, <em>dish_string</em>, <em>restaurant_name</em>, <em>hashid</em>, and cost-related information (taxes, delivery charge, packing charge, discounts) from <em>itemdict</em>.</li>
								<li><strong>Constructing Dataset:</strong> For each order, it constructs a row containing the extracted information. These rows are appended to the <em>dataset</em> list.</li>
								<li><strong>Creating DataFrame and Saving as CSV:</strong> After parsing all orders, it creates a pandas DataFrame from the <em>dataset</em>, specifying column names (<em>datacols</em>). Finally, it saves the DataFrame as a CSV file using <code>df.to_csv()</code>.</li>
								</ul>

								<pre><code># Code snippet for extracting detailed order information
import json
import pprint
import pandas
import re
from dateutil import parser


def process_order_data(datafile, outfile, per_item_file):
    datacols = ['order_date', 'cost', 'payment_status', 'delivery_address', 'dish_string',
                'resturant_name', 'hashid', 'taxes', 'delivery_charge', 'packing_charge', 'discounts']
    dataset = []

    item_json = json.load(open(per_item_file, 'r'))
    itemdict = {}
    for _rec in item_json:
        recdict = {}
        charge_dict = _rec['data']['details']['order']['items']

        taxes = sum([x['totalCost'] for x in charge_dict.get('tax', [])])

        delivery_charge = list(filter(lambda x: x['itemName'] == 'Delivery Charge', charge_dict.get('charge', [])))
        delivery_charge = delivery_charge[0]['totalCost'] if delivery_charge else 0

        packing_charge = list(filter(lambda x: x['itemName'] == 'Restaurant Packaging Charges', charge_dict.get('charge', [])))
        packing_charge = packing_charge[0]['totalCost'] if packing_charge else 0

        if charge_dict.get('voucher_discount'):
            discounts = sum([x['totalCost'] for x in charge_dict.get('voucher_discount')])
        else:
            discounts = 0

        costdict = {
            'taxes': taxes,
            'delivery_charge': delivery_charge,
            'packing_charge': packing_charge,
            'discounts': discounts,
        }

        for k,v in charge_dict.items():
            if k == 'dish':
                continue
            recdict[k] = {}
            for _charge_item in v:
                recdict[k][_charge_item['itemName']] = _charge_item['totalCost']

        itemdict[_rec['hashid']] = costdict


    currency_clean = re.compile(r'[^\d.,]+')

    jsondata = json.load(open(datafile, 'r'))
    for _page in jsondata:
        for k, v in _page['entities']['ORDER'].items():

            hashid = v['hashId']
            row = [parser.parse(v['orderDate']), currency_clean.sub('', v['totalCost']), v['paymentStatus'],
                   v['deliveryDetails']['deliveryAddress'],
                   v['dishString'], v['resInfo']['name'], hashid, itemdict[hashid]['taxes'],
                   itemdict[hashid]['delivery_charge'], itemdict[hashid]['packing_charge'],
                   itemdict[hashid]['discounts']]
            dataset.append(row)

    df = pandas.DataFrame(data=dataset, columns=datacols)
    df.to_csv(outfile, index=False)</code></pre>

								<h3>Lets look at how to generate item information for each order.</h3>
								<p><strong>Function parse_item_data(itemfile, itemoutfile)</strong></p>
									<ul>
									<li><strong>Opening JSON File:</strong> This function takes two arguments: <em>itemfile</em>, the path to the JSON file containing detailed order information per item, and <em>itemoutfile</em>, the path where the resulting CSV file will be saved. It opens the JSON file using <code>open()</code> and loads its contents into memory using <code>json.load()</code>.</li>
									<li><strong>Parsing JSON Data:</strong> It iterates through each record in the JSON data. For each record, it extracts the <em>hashid</em>, which is a unique identifier for the order, and the <em>item_data</em>, which contains detailed information about each item ordered.</li>
									<li><strong>Constructing Dataset:</strong> For each item in the <em>item_data</em>, it constructs a row containing the <em>hashid</em>, <em>itemname</em>, <em>unitcost</em>, <em>totalcost</em>, and <em>quantity</em> of the item. These rows are appended to the <em>dataset</em> list.</li>
									<li><strong>Creating DataFrame and Saving as CSV:</strong> After parsing all records, it creates a pandas DataFrame from the <em>dataset</em> list, specifying column names (<em>datacols</em>). Finally, it saves the DataFrame as a CSV file using <code>df.to_csv()</code>.</li>
								</ul>
									<pre><code># Code snippet for extracting detailed order information
import json
import pprint
import pandas
import re
from dateutil import parser

def parse_item_data(itemfile, itemoutfile):
    f = open(itemfile, 'r')
    datafile = json.load(f)
    datacols = ['hashid', 'itemname', 'unitcost', 'totalcost', 'quantity']
    dataset = []

    for _item in datafile:
        hashid = _item['hashid']
        item_data = _item['data']['details']['order']['items'].get('dish')
        if not item_data:
            continue
        for _dish in item_data:
            row = [hashid, _dish['itemName'], _dish['unitCost'], _dish['totalCost'], _dish['quantity']]
            dataset.append(row)

    df = pandas.DataFrame(data=dataset, columns=datacols)
    df.to_csv(itemoutfile, index=False)</code></pre>

								<hr class="major" />

								<h2>Both functions are called to generate final files.</h2>
<pre><code># Code snippet for calling transformation functions.

if __name__ == "__main__":

    itemdatafile = r'C:\Users\user1\data_per_item.json'
    itemoutfile = r'C:\Users\user1\data_per_item.csv'
    parse_item_data(itemdatafile, itemoutfile)

    datafile1 = r'C:\Users\user1\data.json'
    outfile1 = r'C:\Users\user1\data.csv'
    process_order_data(datafile1, outfile1, itemdatafile)</code></pre>
								
								<hr class="major" />

								<h2>Conclusion</h2>
								<p>The post explores how web api can be used to extract data from websites which make use of such technology, we also looked at some basic transformation to create a dataset which can be easily analysed using tools like Tableau, Power BI, Excel. To checkout further analysis on this dataset, click <a href="project_2.html" target="_blank">here</a>  to checkout my project on this dataset.</p>

								</section>

						</div>
					</div>

				<!-- Sidebar -->
				<div id="sidebar">
					<div class="inner">

						<!-- Search -->
							<section id="search" class="alt">
								<form method="post" action="#">
									<input type="text" name="query" id="query" placeholder="Search" />
								</form>
							</section>

						<!-- Menu -->
							<nav id="menu">
								<header class="major">
									<h2>Menu</h2>
								</header>
								<ul>
									<li><a href="index.html">Homepage</a></li>
									<li><a href="resume.html">Resume</a></li>
									<li>
										<span class="opener">Projects</span>
										<ul>
											<li><a href="salary_cleansing.html">Salary Data Cleansing</a></li>
											<li><a href="project_2.html">Food Delivery Analysis</a></li>
											<li><a href="retail_pipeline.html">UK Retail Data Pipeline</a></li>
											<li><a href="retail_customer_transaction.html">Customer Behavior Analysis</a></li>
										</ul>
									</li>
									<li>
										<span class="opener">Blogs</span>
										<ul>
											<li><a href="blog_sql_question.html">Postgresql Generate Series</a></li>
											<li><a href="blog_scraping_simple_food_orders.html">Simple Scraping and Transformation</a></li>
											<li><a href="blog_job_tracker.html">Job Application Tracking</a></li>
										</ul>
									</li>
								</ul>
							</nav>

						<!-- Section -->
							<section>
								<header class="major">
									<h2>Get in touch</h2>
								</header>
								<p>Do get in touch if you are looking for a professional working in the field of data.</p>
								<ul class="contact">
									<li class="icon solid fa-envelope"><a href="#">milangarg6991@gmail.com</a></li>
									<li class="icon solid fa-phone">(+44) 7741-851570</li>
									<li class="icon solid fa-home">London, UK</li>
								</ul>
							</section>

						<!-- Footer -->
						<footer id="footer">
							<p class="copyright">Demo Images: <a href="https://unsplash.com">Unsplash</a>. Design: <a href="https://html5up.net">HTML5 UP</a>.</p>
						</footer>

					</div>
				</div>

			</div>

		<!-- Scripts -->
			<script src="assets/js/jquery.min.js"></script>
			<script src="assets/js/browser.min.js"></script>
			<script src="assets/js/breakpoints.min.js"></script>
			<script src="assets/js/util.js"></script>
			<script src="assets/js/main.js"></script>

	</body>
</html>